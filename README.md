# Droctor

Source code:

https://drive.google.com/drive/folders/1DMNjC4YuZczNv5RuTDRubLFZrv19wmOi?usp=sharing

Web application that controls a drone tello via buttons and gestures.

![image](https://user-images.githubusercontent.com/26171557/187046075-f6c70b1d-34f8-454f-abec-a453484fe3b3.png)


Layout:

![image](https://user-images.githubusercontent.com/26171557/187046087-1414a079-6d63-4a37-a8fd-c5bdd7319a27.png)


Patients:

![image](https://user-images.githubusercontent.com/26171557/187046094-ac2d93ff-bd48-4ba3-b1b7-d4c5d353b6fe.png)


Visual test:

![image](https://user-images.githubusercontent.com/26171557/187046109-dec1eecb-709d-4296-bae3-00cb67c04952.png)


Geolocation:

![image](https://user-images.githubusercontent.com/26171557/187046155-83323e75-21a8-47ed-ab07-53afa0f30a3d.png)


There are two cameras being used in this multi-agent system, one is the web cam of the computer the other, the camera of the UAV, the first camera helps the doctor to control the flying device and also uses AI for computer vision mechanisms that allow the pilot to see himself, while performing his tasks, the second camera is in the drone, that will treat the Photoplethysmogram (PPG) signal obtained allowing to know the BPM and other important medical information via the face detected by the done camera. The level of light influences the accuracy of the readings, but after tests even in low light environment the levels obtained are almost the same as the one perceived by medical instruments like infra-red (IR) thermometers or oximeters. 
